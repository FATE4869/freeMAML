{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "caleb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FATE4869/freeMAML/blob/master/caleb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIftHEWk6mtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import click\n",
        "import os, sys\n",
        "import numpy as np\n",
        "import random\n",
        "# from setproctitle import setproctitle\n",
        "import inspect\n",
        "import pdb\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Parameter\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.nn.modules.loss import CrossEntropyLoss\n",
        "\n",
        "from task import OmniglotTask, MNISTTask\n",
        "from dataset import Omniglot, MNIST\n",
        "from inner_loop import InnerLoop\n",
        "# from omniglot_net import OmniglotNet\n",
        "from score import *\n",
        "from data_loading import *\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "from layers import *\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKrz1HoG8wpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OmniglotNet(nn.Module):\n",
        "    '''\n",
        "    The base model for few-shot learning on Omniglot\n",
        "    '''\n",
        "\n",
        "    def __init__(self, num_classes, loss_fn, num_in_channels=3):\n",
        "        super(OmniglotNet, self).__init__()\n",
        "        # Define the network\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "                ('conv1', nn.Conv2d(num_in_channels, 64, 3)),\n",
        "                ('bn1', nn.BatchNorm2d(64, momentum=1, affine=True)),\n",
        "                ('relu1', nn.ReLU(inplace=True)),\n",
        "                ('pool1', nn.MaxPool2d(2,2)),\n",
        "                ('conv2', nn.Conv2d(64,64,3)),\n",
        "                ('bn2', nn.BatchNorm2d(64, momentum=1, affine=True)),\n",
        "                ('relu2', nn.ReLU(inplace=True)),\n",
        "                ('pool2', nn.MaxPool2d(2,2)),\n",
        "                ('conv3', nn.Conv2d(64,64,3)),\n",
        "                ('bn3', nn.BatchNorm2d(64, momentum=1, affine=True)),\n",
        "                ('relu3', nn.ReLU(inplace=True)),\n",
        "                ('pool3', nn.MaxPool2d(2,2))\n",
        "                # ('conv4', nn.Conv2d(64,64,3)),\n",
        "                # ('bn4', nn.BatchNorm2d(64, momentum=1, affine=True)),\n",
        "                # ('relu4', nn.ReLU(inplace=True)),\n",
        "                # ('pool4', nn.MaxPool2d(2,2))\n",
        "        ]))\n",
        "        self.add_module('fc', nn.Linear(64, num_classes))\n",
        "        \n",
        "        # Define loss function\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x, weights=None):\n",
        "        ''' Define what happens to data in the net '''\n",
        "        if weights == None:\n",
        "            \n",
        "            x = self.features(x)\n",
        "            x = x.view(x.size(0), 64)\n",
        "            x = self.fc(x)\n",
        "        else:\n",
        "            x = conv2d(x, weights['features.conv1.weight'], weights['features.conv1.bias'])\n",
        "            x = batchnorm(x, weight = weights['features.bn1.weight'], bias = weights['features.bn1.bias'], momentum=1)\n",
        "            x = relu(x)\n",
        "            x = maxpool(x, kernel_size=2, stride=2) \n",
        "            x = conv2d(x, weights['features.conv2.weight'], weights['features.conv2.bias'])\n",
        "            x = batchnorm(x, weight = weights['features.bn2.weight'], bias = weights['features.bn2.bias'], momentum=1)\n",
        "            x = relu(x)\n",
        "            x = maxpool(x, kernel_size=2, stride=2) \n",
        "            x = conv2d(x, weights['features.conv3.weight'], weights['features.conv3.bias'])\n",
        "            x = batchnorm(x, weight = weights['features.bn3.weight'], bias = weights['features.bn3.bias'], momentum=1)\n",
        "            x = relu(x)\n",
        "            x = maxpool(x, kernel_size=2, stride=2) \n",
        "            # x = conv2d(x, weights['features.conv4.weight'], weights['features.conv4.bias'])\n",
        "            # x = batchnorm(x, weight = weights['features.bn4.weight'], bias = weights['features.bn4.bias'], momentum=1)\n",
        "            # x = relu(x)\n",
        "            # x = maxpool(x, kernel_size=2, stride=2) \n",
        "            x = x.view(x.size(0), 64)\n",
        "            x = linear(x, weights['fc.weight'], weights['fc.bias'])\n",
        "\n",
        "        return x\n",
        "\n",
        "    def net_forward(self, x, weights=None):\n",
        "        return self.forward(x, weights)\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        ''' Set weights to Gaussian, biases to zero '''\n",
        "        torch.manual_seed(1337)\n",
        "        torch.cuda.manual_seed(1337)\n",
        "        torch.cuda.manual_seed_all(1337)\n",
        "        # print ('init weights')\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                n = m.weight.size(1)\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                #m.bias.data.zero_() + 1\n",
        "                m.bias.data = torch.ones(m.bias.data.size())\n",
        "    \n",
        "    def copy_weights(self, net):\n",
        "        ''' Set this module's weights to be the same as those of 'net' '''\n",
        "        # TODO: breaks if nets are not identical\n",
        "        # TODO: won't copy buffers, e.g. for batch norm\n",
        "        for m_from, m_to in zip(net.modules(), self.modules()):\n",
        "            if isinstance(m_to, nn.Linear) or isinstance(m_to, nn.Conv2d) or isinstance(m_to, nn.BatchNorm2d):\n",
        "                m_to.weight.data = m_from.weight.data.clone()\n",
        "                if m_to.bias is not None:\n",
        "                    m_to.bias.data = m_from.bias.data.clone()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWzQw06CQz9",
        "colab_type": "code",
        "outputId": "0f1f40ef-2993-4599-fe10-23aed41f0c3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset,DataLoader,sampler\n",
        "# from torch.utils.data.sampler import SubsetRandomSampler\n",
        "dataset = torchvision.datasets.Omniglot(\n",
        "    root=\"./data\", download=True, transform=torchvision.transforms.ToTensor()\n",
        ")\n",
        "image, label = dataset[0]\n",
        "# print(type(image))  # torch.Tensor\n",
        "# print(type(label))  # int\n",
        "for i in range(10):\n",
        "    image, label = dataset.__getitem__(i)\n",
        "    # print(image.shape)\n",
        "    # print(label)\n",
        "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "data_iter = iter(data_loader)\n",
        "for i in range(10):\n",
        "    # print(i)\n",
        "    image,label = next(data_iter)\n",
        "    # print(image.shape)\n",
        "    # print(label)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oxijBC37GCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_task():\n",
        "  root = './data/omniglot-py'\n",
        "  split='train'\n",
        "  n_cl = 20\n",
        "  n_inst = 1\n",
        "  return OmniglotTask(root, n_cl, n_inst, split)\n",
        "\n",
        "def Hvp(net, batch_X, batch_y, v):\n",
        "    \"\"\"\n",
        "    Computes an Hessian-vector product using the Pearlmutter method.\n",
        "    The Hessian used is the loss function of tf.keras.Model 'model' with respect to its trainable_variables.\n",
        "    The loss is evaluated on the given batch.\n",
        "    The vector v is decomposed into a list of Tensors, to match the number of weights of 'model' and their shape.\n",
        "    \"\"\"\n",
        "    loss, _ = net.forward_pass(batch_X, batch_y)\n",
        "    # for name, param in net.parameters():\n",
        "    for name, param  in net.parameters():\n",
        "      print(name, param.shape)\n",
        "    grad = torch.autograd.grad(loss, net.parameters())\n",
        "    \n",
        "    for name, param  in net.parameters():\n",
        "      print(name, param.shape)\n",
        "    Hv = torch.autograd.grad(grad, net.parameters(), v)\n",
        "\n",
        "    return Hv\n",
        "\n",
        "def grads_on_batch(net, batch_X, batch_y):\n",
        "    loss, _ = net.forward_pass(batch_X, batch_y)\n",
        "\n",
        "    grads = torch.autograd.grad(loss, net.parameters(), create_graph=True)\n",
        "    return grads\n",
        "\n",
        "# Optimizers that can be used to approximate the meta-gradient (I+1/lambda*H)*g = b -> Ag = b\n",
        "# A is supplied as a linear operator function, taking a vector as argument and returning the matrix-vector product.\n",
        "def plain_gradient_descent(Av, b, x0, num_iterations, learning_rate=0.01, debug=False):\n",
        "    ## Gradient descent\n",
        "    for i in range(num_iterations):\n",
        "        Ax = Av(x0)\n",
        "        for k in range(len(x0)):\n",
        "            x0[k] = x0[k] - 0.01 * (Ax[k]-b[k])\n",
        "\n",
        "        if debug:\n",
        "            print(f1(x0, Av, b), f2(x0, Av, b) , '\\n\\n')\n",
        "\n",
        "    return x0\n",
        "\n",
        "\n",
        "class InnerLoop(OmniglotNet):\n",
        "    \n",
        "    def __init__(self, num_classes, loss_fn, num_updates, step_size, batch_size, meta_batch_size, num_in_channels = 3):\n",
        "        super(InnerLoop, self).__init__(num_classes, loss_fn, num_in_channels)\n",
        "\n",
        "        # Number of updates to be taken\n",
        "        self.num_updates = num_updates\n",
        "\n",
        "        # Step size for the update\n",
        "        self.batch_size = meta_batch_size\n",
        "\n",
        "        # Per class batch size for the updates\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # for loss normalization\n",
        "        self.meta_batch_size = meta_batch_size\n",
        "\n",
        "        # sth\n",
        "        self.n_iters_optimizer = 1\n",
        "        # print(self.parameters())\n",
        "    def initialize(self):\n",
        "        \"\"\"\n",
        "        This method should be called after the wrapped model is compiled.\n",
        "        \"\"\"\n",
        "        self.current_initial_parameters = [v for (k,v) in net.named_parameters() ]\n",
        "        return self.current_initial_parameters\n",
        "\n",
        "    def task_begin(self, current_initial_parameters, task = None, **kwargs):\n",
        "        with torch.no_grad():\n",
        "\n",
        "          count = 0\n",
        "          for name, param in net.named_parameters():\n",
        "            param.copy_(current_initial_parameters[count])\n",
        "            count += 1\n",
        "         \n",
        "    def task_end(task=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Method to call after training on each meta-batch task; possibly return relevant information for the\n",
        "        meta-learner to use for the meta-updates.\n",
        "        \"\"\"\n",
        "        assert task is not None, \"FOMAML needs a `task' argument on .task_end to compute the data it needs.\"\n",
        "\n",
        "        # Compute the gradient of the TEST LOSS evaluated at the final parameters. E.g., mean gradients\n",
        "        # over a batch of test data\n",
        "\n",
        "        ret_grads = gradients_for_task_CG(task)\n",
        "\n",
        "        return ret_grads\n",
        "    def forward_pass(self, in_, target, weights=None):\n",
        "        ''' Run data through net, return loss and output '''\n",
        "        input_var = torch.autograd.Variable(in_)\n",
        "        target_var = torch.autograd.Variable(target)\n",
        "        # Run the batch through the net, compute loss\n",
        "        out = self.net_forward(input_var, weights)\n",
        "        loss = self.loss_fn(out, target_var)\n",
        "        return loss, out\n",
        "\n",
        "    def gradients_for_task_CG(self, task = 'omniglot'):\n",
        "        batch = 1\n",
        "        train_loader = get_data_loader(task, batch, 'train')\n",
        "        test_x, test_y = train_loader.__iter__().next()\n",
        "        train_x, train_y = train_loader.__iter__().next()\n",
        "\n",
        "        def Av(v):\n",
        "            Hv = Hvp(net, train_x, train_y, v)\n",
        "            return [v[i] + 1.0/1.0 * Hv[i].numpy() for i in range(len(v))]\n",
        "\n",
        "        test_gradients = [g.detach().numpy() for g in grads_on_batch(self, test_x, test_y)]\n",
        "\n",
        "        x0 = [np.zeros(var.shape, dtype = np.float32) for var in self.current_initial_parameters]\n",
        "\n",
        "        b = copy.deepcopy(test_gradients)\n",
        "\n",
        "        x = plain_gradient_descent(Av, b, x0, self.n_iters_optimizer, learning_rate=0.01)\n",
        "\n",
        "        return x\n",
        "    def update(self, list_of_final_gradients, **kwargs):\n",
        "        \"\"\"\n",
        "        Main (outer-loop) training operation. Call after training on a whole meta-batch, after each meta-iteration\n",
        "        has finished.\n",
        "        \"\"\"\n",
        "        # Perform a FOMAML update for the outer meta-training iteration. Take the expected test gradient\n",
        "        # L'(\\tilde{phi}) over tasks in the meta-batch and perform a single step of gradient descent.\n",
        "        avg_final_grads = []\n",
        "        for grads in zip(*list_of_final_gradients):\n",
        "            avg_final_grads.append(np.mean(grads, axis=0))\n",
        "\n",
        "        optimizer = torch.optim.SGD(self.named_parameters(), lr=1e-4)\n",
        "\n",
        "        # Apply gradients to the *initial parameters*\n",
        "        for i in range(len(self.current_initial_parameters)):\n",
        "            self.named_parameters[i].assign( self.current_initial_parameters[i] )\n",
        "            self.named_parameters[i].grad = avg_final_grads[i]\n",
        "\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        # Set the new initial parameters\n",
        "        self.current_initial_parameters = [v.numpy() for v in self.named_parameters] \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxubedrV7KfT",
        "colab_type": "code",
        "outputId": "ff0ee657-17fd-4e34-b53f-5ba7f178b596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "dataset='omniglot'\n",
        "num_classes= 20\n",
        "num_inst=1\n",
        "batch_size = 1\n",
        "meta_batch_size = 16\n",
        "num_updates=1\n",
        "num_inner_updates=1\n",
        "num_in_channels = 3\n",
        "step_size='1e-1'\n",
        "inner_lr = 0.1\n",
        "meta_lr='1e-3'\n",
        "gpu= 0\n",
        "input_size = 28\n",
        "loss_fn = CrossEntropyLoss()\n",
        "\n",
        "net = InnerLoop(num_classes, loss_fn, num_updates, step_size, batch_size, meta_batch_size, num_in_channels)\n",
        "current_initial_parameters = net.initialize()\n",
        "task = get_task()\n",
        "\n",
        "net.task_begin(current_initial_parameters)\n",
        "\n",
        "inps = torch.rand(batch_size, num_in_channels, input_size, input_size, requires_grad=False)\n",
        "targets = torch.randint(0, num_classes, (batch_size,))\n",
        "# net.train()\n",
        "\n",
        "\n",
        "# loss, out = net.forward_pass(inps, targets, weights=None)\n",
        "x = net.gradients_for_task_CG(task)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init weights\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-151-9f1071562f1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# loss, out = net.forward_pass(inps, targets, weights=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients_for_task_CG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-150-c5d6765db184>\u001b[0m in \u001b[0;36mgradients_for_task_CG\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_gradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplain_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iters_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-150-c5d6765db184>\u001b[0m in \u001b[0;36mplain_gradient_descent\u001b[0;34m(Av, b, x0, num_iterations, learning_rate, debug)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m## Gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mAx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mx0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-150-c5d6765db184>\u001b[0m in \u001b[0;36mAv\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mAv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mHv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mHv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-150-c5d6765db184>\u001b[0m in \u001b[0;36mHvp\u001b[0;34m(net, batch_X, batch_y, v)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# for name, param in net.parameters():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZHluQ1TtRTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c07gz77h7ttv",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}